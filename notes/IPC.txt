When the VFS gets a request, store all the relevant data on the stack. Don't hold locks when calling into lower layers.
	When the owner of the thread dies. Signal any blocking operation and return an error that the owner died so it should just chain that error up.
The cost of threads are associated with the user process that spawned it.
	Associate data stacks with the cost too during context switches?
		Sounds expensive and the cost might be factor of the call stack cost anyway.
		Perhaps there should be a memory allocator which associates the data with the thread owner, for data which is not a factor of the call stack cost.

Do manual overflow checks on data stack
	Allocate data stack when context switching with a freelist?
		Use a global data stack address space that is mapped into each process
			Address space allocations are global, but only the local process gets access to the stack
			This allows the allocator to be in the kernel and have great locality - we don't want cache misses during context switches!
			Requires small stack allocations so we don't run out of memory. So basically requires segmented stacks and atleast 4K stack size.

			We can thread stacks so no adjacent stacks belong to the same process. Full address space utilization and we can rely on page faults
			for stack overflow checking << DO THIS

				Allocate a 4K stack during context switching and allocate a larger stack when the probing reaches a trap page

				r15 = stack pointer

				prologue:
					sub r15, <stack-frame-size>
					or [r15], 0

			Might be better with just local process allocators which is managed by the kernel so they are still cache friendly.
				Would prevent DOS attacks where some local process eats up all the global stacks.

				Reuse stack for the current thread and don't allocate on each context switch
					Would prevent DOS attacks from a single thread
						But this can also be done by not calling back into the owner

				Use a global stack area as a cache
					Give each process an offset for where this global area maps to
					When all the stacks in a process is allocated do an proper LOCKED allocation and place the stack area there
						Must update the offset of all processes offset so make it global,
						do so lazily with a global area modification counter? Which would do it globally on overflow
						Requires a branch
					We can garbage collect the global area by remapping so that each process has no stacks allocated in it. (If that is needed)
					The global stack area should be CPU local

					How to quickly free?
						If the area offset stayed the same for the process, we can just unmark it otherwise we must unmark it in the process allocator
						Requires 2 atomic operations and a branch, not ideal
							We can mark it as allocated with regular operation (since we own the area, so other threads can't interrupt),
							that way we get one OR op to mark and a atomic free operation (load/ATOMIC store loop)
								Avoid the atomic operation by aligning the global area to a byte in bitmap form.
								So each area offset can't overlap
								How does that prevent the area from being reused? It does seem to
								We can avoid the atomic op by comparing the modification counter and only do an atomic if it changed

								We can also avoid atomic ops by requiring that the local area is split into CPU local parts

					We must avoid marking and freeing from the local area bitmap, otherwise just using the local area bitmap directly is more efficient

					Use a counter 'n' for the global area and have stacks indexed by < n be marked as allocated. This counter is thread-local and is saved by interrupt context switches. (or it could be CPU-local, possibly)
						When n overflows we allocate enough space for another counter from the global area. We could also just use a slow path here if thread-local since the thread might be misbehaving.

					Can we design an malloc like this too? Probably poor locality
						Could be useful for mmap?

					Map all stacks to physical memory to avoid page faults. Unused pages can be reclaimed if the system is low on memory.

			Does require freeing the memory also on each context switch return! - can we garbage collect these stacks somehow?
				Garbage collection might be too slow, and freeing is likely to touch cache anway


	When context switching set the data stack to zero which triggers the overflow check and ensures an allocation is done?
		Expensive code size overhead from branches
			cheap segmented stacks?

				r15 = stack pointer
				r14 = stack end

				prologue:
					sub r15, <stack-frame-size>
					cmp r15, r14
					jnb ALLOC
					call alloc_stack // this requires it to be part of the prologue and not just be plain x86 code (a LLVM pass to add the split-stack attribute would probably work)
				ALLOC:
	Pass the data stack in a callee save register?

Do lazy allocations for TLS

Message passing stack allocation with block indexed by 'i' with 'n' stacks - CPU Local
block_index = version of block

	i += 1
	On overflow ->
		block = allocate new block with size 'n'
		block_index += 1
			On overflow -> Do a slow path to reset block_index

	stack = block[i]
	i -= 1

Message passing stack allocation with block indexed by 'i' with 'n' stacks - Thread Local
	i += 1
	On overflow ->
		stack = slow path which uses a bitmap per process
		i -= 1 // Cause overflow and the slow path next time too, so we can allocate in a different way
		call
		free stack
	stack = block[i]
	call
	i -= 1

	actual code - stack passed in r15
		add r15, 0x2000 // increase by two pages (skipping the guard page)
		cmp fs:[stack_end], r15  // compare stack with stack end, if the stack is expanded from 4K, this is set to 0 so it always triggers
		jbe SLOW_PATH
		call target
		sub r15, 0x2000

	SLOW_PATH:
		mov r15, fs:[old_stack] // Load the old indexed stack
		jz NO_INDEXED
		mov r14, fs:[old_stack_end] // Load the old stack end
		mov fs:[stack_end], r14
		add r15, 0x2000
		call target
		sub r15, 0x2000
	NO_INDEXED:
		...

	'block' and 'i' is thread-local

	Threads with even ID allocates even (2i) stacks
	Threads with odd ID allocates odd (2i + 1) stacks

	Processes which allocates lots of threads will use something slower

	Reserving 8 GB and 8 stacks per thread will give 256k threads so this is sufficient

Have diverging functions in Rust which drops all variables on the frame
Have diverging IPC which can only return to some higher non-diverging IPC call

What IPC must do:
	Check that there is free call stack space, return an error if not (could call something to allocate more space and use segmented stacks)
	Check that the target server is swapped in. Mark is as active.
	Switch GS segment to the target server
	Ensure that the caller has access to the server, return an error if not
	Call the server IPC entry point
	Switch the GS segment back

Exceptions and IRQs should use async seL4-like IPC

Store capabilities and excess messages in a TCB buffer. It can only be accessed by the current process the thread is in
	Allow capability store methods to directly address this buffer?

Store the function to call in the endpoint?
	So endpoints = (Process, Function, Badge)
	Process = 32bit pointer into kernel heap
	Store a process/function pair into a kernel object? (have that be the endpoint instead?)
	Endpoint = (Process, Function)
	Object = (Endpoint, Badge)
	Add a syscall to extract Function, Badge given (Process, Object)?

Have 'Portal' capabilties which is the only capabiltiy which can be invoked
	The 'Portal' capability stores the process and the function to call when invoked.

	'ObjectType' is a kernel object which stores a process ID and a word.

	The 'Object' capability stores an ObjectType pointer and a word. <- Could store a 32-bit process ID and a 32-bit type ID?

	A syscall extracts the ObjectType word and the Object word given an Object if the current process owns the ObjectType.

	('ObjectType' might also store a descriptive string, for debugging purposes)

	A process can create Portal, ObjectType and Object capabilties at will.

	Doesn't work for talking to different server using the same interface....

An Interface stores the process, a word and an array of functions to call.
	An Object stores an Interface pointer and a word.
	A RPC takes an Object capability and a index into the function arra, and calls the selected function. It passes the Object word as an argument to the called function.
	A syscall extracts the Interface word and the Object word given an Object if the current process owns the Interface.

Any capabilties management happens through direct syscalls and not using IPC. Other kernel services should use IPC.

<Zoxc> I think I have capabilities for my RPC figured out.
<Zoxc> The 'Portal' capability stores the process and the function to call when invoked.
<Zoxc> The ELF loader will set those up so each Portal corresponds to a function to call in a server. (Like file_open, file_close, etc.)
<Zoxc> 'ObjectType' is a kernel object which stores a process ID and a word.
<Zoxc> The 'Object' capability stores an ObjectType pointer and a word.
<Zoxc> A syscall extracts the ObjectType word and the Object word given an Object if the current process owns the ObjectType.
<Zoxc> A process can create Portal, ObjectType and Object capabilties at will.
<Zoxc> There is a fixed size capability store in the TCB. The current process can access that store.
<Zoxc> Since the thread doesn't change during a RPC, only the process. That can be used to transfer capabilities.

Object words can hold pointers, and server can encrypt those pointers to avoid leaking addresses.
How can we identify clients in this scheme?
Do we need to identify clients? Can we just pass along a process ID?

How to free data for clients when we run out of resources or processes exit?
How can a server free resources belonging to a process if that process exits? Given that a client can transfer capabilities to another process without informing the server

Have a buffer object which can reference some other process' pages and a journal of changes. Rope structure?

Can we avoid allocating slots for capabilities when doing IPC?
	Store capabilities on the stack and pass a pointer to them to the IPC dispatcher?
	We can't easily

	Pass along a buffer on the stack to the IPC dispatcher. Analyzer will ensure it's valid.
	IPC dispatcher will fill it with the passed capability pointers. It will save the TCB pointer to the capability buffer on the stack.
	It will then set the TCB pointer to the capability buffer to the buffer on the stack.
	If we disallow the same process to call itself twice. We can refer to the capabilities with a LocalCap<'c> lifetime where the entry point is generic in 'c.
		If we have a limited stack of pointers to capabilities buffers, we can allow reentrancy - very ugly

	How will this work on the Mill?

	<Zoxc> I was thinking of adding a ThreadLocalCap<'s> type, which points to a capability in the TCB
	<Zoxc> RPC entry points are generic in 's and produces instances of that type by overwritting the capabilities in the TCB
	<Zoxc> Would it be possible for a RPC entry points to access ThreadLocalCap of a entry point higher up in the stack?
		Could transfer the ThreadLocalCap as raw bytes to the RPC below, but that requires unsafe and is unusual.

	Can the TCB store a pointer into the capability storage of the caller?
		Can be used for input/output

		Separate input/output so the server can't read output slots?
			Also could prevent server from changing inputs, allowing those to be reused

		Pass in the input count and zero the rest of the capabilities?

Make it easy to revokate an Object by having copies be pointers to the original?
	Could be unified with server heap dellocation of objects?


How will indirect Endpoints work on the Mill? It only has hardcoded portals?
	Each Endpoint will create a Portal in a IPC service which has access to capabilities.
	Each process has a Portal into the IPC server to do a RPC.
	The function pointed to by the Endpoint will dispatch to the correct method in the server.

	The IPC server can store passed capabilities on its stack

	Could Mill have a separate CSpace capability on memory, portal calls could take an unforgable argument from there?

During a portal call, how can we ensure that there's enough spiller space?

How are capabilities allocated in hardware with grant?
	How can we ensure that the caller has access to the pass()-ed or granted capability with a RPC?